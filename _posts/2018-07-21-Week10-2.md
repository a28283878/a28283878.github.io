---
layout: post
title: "ML --- Week 10-2(Online Learning)"
author: "Richard Lin"
categories: ml
tags: ml
image:
  feature: ml.jpg
---
菜雞學ML

## Online Learning
* * *
這個就類似stochastic的概念，預設資料室由stream進來的(因為是網頁的關係)，我們不要一次收集大量資料再處理，反而是向Stochastic一樣一有資料我們就更新一次
`
\theta
`
。
<img src="../assets/img/ml/w10_13.jpg" style="width:100%; border-radius:10px; padding:5px 0 5px 0;">

## Map reduce
* * *
這個就很像是mini-batch的觀念。<br>
我們將資料拆分成不同區段並給不同電腦計算，再來我再將資料整合做iteration。<br>
以Batch gradient descent為例子，假設我們有四台電腦，我們就將資料拆成四部分去算J的導數，最後再將導數都合併去更新
`
\theta
`
<img src="../assets/img/ml/w10_14.jpg" style="width:100%; border-radius:10px; padding:5px 0 5px 0;">
這邊可以適用的演算法只要是其衝可以拆解成什麼跟什麼的和，就都可以套用map reduce這個觀念，將資料拆解運算。
<img src="../assets/img/ml/w10_15.jpg" style="width:100%; border-radius:10px; padding:5px 0 5px 0;">





