---
layout: post
title: "ML --- Week 3-2"
author: "Richard Lin"
categories: ml
tags: ml
image:
  feature: ml.jpg
---
菜雞學ML

## Overfitting
* * *
當我們用太多的feature去取得假設函數的時候，可能會造成過度耦合的問題。<br>
導致在training set非常準，可是到預測的時候就會一蹋糊塗。
<img src="../assets/img/ml/w3_12.jpg" style="width:90%; border-radius:10px; padding:5px 0 5px 0;">
<img src="../assets/img/ml/w3_13.jpg" style="width:90%; border-radius:10px; padding:5px 0 5px 0;">
這時可以有兩種解決方式：
1. 減少features。
2. Regularization做正規化。
<img src="../assets/img/ml/w3_14.jpg" style="width:90%; border-radius:10px; padding:5px 0 5px 0;">


## Regularization
* * *
直觀的解釋就是，我們可以透過"懲罰"theta來得到更圓滑的曲線(像是右邊的桃紅色線)
<img src="../assets/img/ml/w3_15.jpg" style="width:90%; border-radius:10px; padding:5px 0 5px 0;">
所以Regulariztion所做的就是將theta都縮小，讓最後跑出來的曲線不會過度耦合。
<img src="../assets/img/ml/w3_16.jpg" style="width:90%; border-radius:10px; padding:5px 0 5px 0;">
<img src="https://latex.codecogs.com/svg.latex?\LARGE&space;\lambda&space;\sum_{j=1}^{n}\Theta&space;_{j}^{2}" title="\LARGE \lambda \sum_{j=1}^{n}\Theta _{j}^{2}" />
這就是我們的正規化函數，它的作用就是將每個theta都平方相加在乘上Lambda(正規化參數)。
<br><br>
### 正規化參數 regularization parameter
它的作用就是平衡左邊原本的Cost Function以及右邊的正規化函數。<br>
若是Lambda太小就會造成Theta還是偏大，導致over fitting的問題繼續存在。<br>
但若Lambda太大就會造成Theta太小，假設函數變得太單調導致High bias(uderfit)的問題。
<img src="../assets/img/ml/w3_17.jpg" style="width:90%; border-radius:10px; padding:5px 0 5px 0;">

## Regularized Linear Regression
* * *
<img src="../assets/img/ml/w3_18.jpg" style="width:90%; border-radius:10px; padding:5px 0 5px 0;">
上面的Cost Function就是linear regression的Regularized Cost Function。<br>
那如果我們要做Gradient Descent的話該怎麼做呢?<br>
一樣是對Cost Function作微分，另外我們將theta(0)分開來寫，因為theta(0)所乘上的X(0)會一直是1所以我們不用懲罰他。而對剩下的Cost Function做微分就會是。因為微分好難，影片中也沒有解釋所以就這樣吧。
<img src="../assets/img/ml/w3_20.jpg" style="width:80%; border-radius:10px; padding:5px 0 5px 0;">
最後套上Gradient Descent的結果就會是如下。
<img src="../assets/img/ml/w3_19.jpg" style="width:90%; border-radius:10px; padding:5px 0 5px 0;">

## Regularized Nornal Equation
* * *
對Normal Equation做正規化只要加上一個矩陣L。
<img src="https://latex.codecogs.com/svg.latex?\inline&space;\LARGE&space;\Theta&space;=(X^{T}X&plus;\lambda\cdot&space;L)^{-1}X^{T}y" title="\LARGE \Theta =(X^{T}X+\lambda\cdot L)^{-1}X^{T}y" />
<img src="https://latex.codecogs.com/svg.latex?\inline&space;\LARGE&space;where&space;\&space;\&space;L&space;=&space;\begin{bmatrix}&space;0&space;&&space;&&space;&&space;&&space;\\&space;&&space;1&space;&&space;&&space;&\\&space;&&space;&&space;1&space;&&space;&\\&space;&&space;&&space;&&space;...&space;&\\&space;&&space;&&space;&&space;&&space;1&space;\end{bmatrix}" title="\LARGE where \ \ L = \begin{bmatrix} 0 & & & & \\ & 1 & & &\\ & & 1 & &\\ & & & ... &\\ & & & & 1 \end{bmatrix}" />
若原本的X^TX是不可逆的矩陣，在加了這個L之後也會變成可逆。

## Regularized Logistic Regression
* * *
跟Regularized Linear Regression差不多。一樣加入正規化函數。
<img src="../assets/img/ml/w3_21.jpg" style="width:90%; border-radius:10px; padding:5px 0 5px 0;">
<img src="../assets/img/ml/w3_22.jpg" style="width:90%; border-radius:10px; padding:5px 0 5px 0;">












